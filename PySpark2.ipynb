{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNo0Z7ITlU4OfO/oSFzerZm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hUpAjTEum6-Z"},"outputs":[],"source":["!pip install pyspark"]},{"cell_type":"code","source":["from pyspark import SparkConf,SparkContext\n","conf=SparkConf().setAppName('abc').setMaster('local') #\n","sc=SparkContext(conf=conf)\n","sc.setLogLevel('ERROR')\n","from pyspark.sql import SparkSession\n","spark=SparkSession.builder.appName('abc').config('','').getOrCreate()\n","# NUMPY Dense Vector\n","import numpy as np\n","v1=np.array([1,2,3,4,5])\n","print(v1)\n","# simple python list\n","v2=[1,2,3,4,5,6]\n","print(v2)\n","# Sparce & dense spark vector\n","from pyspark.mllib.linalg import Vectors\n","v3=Vectors.dense([3,4,5,6])\n","print(v3)\n","v4 = Vectors.sparse(3, [0, 2], [1.0, 3.0])\n","print(v4)"],"metadata":{"id":"K6lIb8DVnGWY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","df = spark.read.csv('/content/drive/MyDrive/ColabInputs/bank.csv', header = True, inferSchema = True)\n","df.printSchema()"],"metadata":{"id":"OZTplcqSnH9P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.show(5,0)"],"metadata":{"id":"MjTyvsB4nLA6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.count()"],"metadata":{"id":"EypyP2jdpNQo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']\n","df.select(numeric_features).describe().toPandas().transpose()"],"metadata":{"id":"Q8-BWw2KpaQW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numeric_data = df.select(numeric_features).toPandas()\n","import seaborn as sns\n","sns.pairplot(numeric_data)"],"metadata":{"id":"tz-48ig1pcDN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# remove age and month\n","df = df.select('age', 'job', 'marital', 'education', 'default', 'balance','housing', 'loan', 'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit')\n","\n","cols = df.columns\n","df.printSchema()"],"metadata":{"id":"u45CT5gOpeHD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n","categoricalColumns = ['job', 'marital', 'education', 'default', 'housing','loan', 'contact', 'poutcome']\n","stages = []\n","for categoricalCol in categoricalColumns:\n","  stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n","  encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n","  stages += [stringIndexer, encoder]\n","label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\n","stages += [label_stringIdx]\n","numericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n","assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n","assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n","stages += [assembler]"],"metadata":{"id":"2fZ-w80EpgBI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["assemblerInputs"],"metadata":{"id":"0RW_sy5IpicL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stages"],"metadata":{"id":"KAzDo_rGpj4F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n","pipeline = Pipeline(stages = stages)\n","pipelineModel = pipeline.fit(df)\n","df = pipelineModel.transform(df)\n","selectedCols = ['label', 'features'] + cols\n","df = df.select(selectedCols)\n","df.printSchema()"],"metadata":{"id":"aRoD4nVZpmJi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.show(5,0)"],"metadata":{"id":"Z7mlopFXqZJ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.select(['label','features']).show(5,0)"],"metadata":{"id":"v0h4TG_kqbW0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","df2=pd.DataFrame(df.take(5),columns=df.columns).iloc[:,:2]\n","pd.set_option('display.max_colwidth', None)\n","print(df2)"],"metadata":{"id":"3pw5MBCSqdWn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df.count())\n","train, test = df.randomSplit([0.7, 0.3], seed = 123)\n","print(\"Training Dataset Count: \" + str(train.count()))\n","print(\"Test Dataset Count: \" + str(test.count()))"],"metadata":{"id":"Ha-FqBSBqgYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Logistic Regression Model\n","from pyspark.ml.classification import LogisticRegression\n","lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n","lrModel = lr.fit(train)"],"metadata":{"id":"Jkjn5fwBqnMI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lrModel.coefficients"],"metadata":{"id":"DqZ6XQxUqrHA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainingSummary = lrModel.summary"],"metadata":{"id":"CSPv6466qtrM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["roc = trainingSummary.roc.toPandas()\n","import matplotlib.pyplot as plt\n","plt.plot(roc['FPR'],roc['TPR'])\n","plt.ylabel('False Positive Rate')\n","plt.xlabel('True Positive Rate')\n","plt.title('ROC Curve')\n","plt.show()\n","print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"],"metadata":{"id":"-ODcjIHCqwGO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"U6kD0PmdqyAC"},"execution_count":null,"outputs":[]}]}